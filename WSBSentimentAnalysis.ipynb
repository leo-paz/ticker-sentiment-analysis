{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"WSBSentimentAnalysis.ipynb","provenance":[{"file_id":"1d93kxf19xOWBBdxfhvZ6J_LgD_ojCe0d","timestamp":1616621554271}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"VLo1ACGj_kWO"},"source":["!pip install numerapi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hiq8AA2Xsx1j"},"source":["!pip install praw\n","!pip install vaderSentiment\n","!pip install ffn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fr3oGWNS-oKD"},"source":["import gc\n","import re \n","import csv\n","import json\n","import time\n","import datetime\n","import requests\n","from tqdm.auto import tqdm\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sn\n","import matplotlib.pyplot as plt\n","\n","import praw #reddit data api\n","import ffn #for loading financial data\n","import numerapi #for numerai tickers\n","\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer #VADER sentiment model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vGWXTBbspHoI"},"source":["%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_i9soE7_HlR"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","tf.test.gpu_device_name()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GrrEFxCW-v-1"},"source":["# Data Collection"]},{"cell_type":"markdown","metadata":{"id":"to-nEtMn_Oo4"},"source":["### Tickers we want\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"dFyhCdi9_HKW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616627496537,"user_tz":240,"elapsed":2079,"user":{"displayName":"leo paz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgO8YWcRnwo2RcIun_aQlHKKD0ymAPBNTZEyRyq=s64","userId":"06657026634474379645"}},"outputId":"8bb7e257-27e7-4be4-e75f-e22dd88c8159"},"source":["napi = numerapi.SignalsAPI()\n","\n","eligible_tickers = pd.Series(napi.ticker_universe(), name=\"bloomberg_ticker\")\n","print(f\"Number of eligible tickers : {len(eligible_tickers)}\")\n","print(eligible_tickers.head(10))\n","\n","ticker_map = pd.read_csv(\n","        'https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_ticker_map_w_bbg.csv'\n",")\n","print(len(ticker_map))\n","\n","#Yahoo <-> Bloomberg mapping\n","yfinance_tickers = eligible_tickers.map(\n","        dict(zip(ticker_map[\"bloomberg_ticker\"], ticker_map[\"yahoo\"]))\n","    ).dropna()\n","\n","bloomberg_tickers = ticker_map[\"bloomberg_ticker\"]\n","print(f\"Number of eligible, mapped tickers: {len(yfinance_tickers)}\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of eligible tickers : 5433\n","0    SVW AU\n","1    GEM AU\n","2    VOC AU\n","3    AZJ AU\n","4    MLD AU\n","5    NXT AU\n","6    TWE AU\n","7    SGR AU\n","8    CKF AU\n","9    BGA AU\n","Name: bloomberg_ticker, dtype: object\n","5433\n","Number of eligible, mapped tickers: 5383\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nBWDw6c9_J-f"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EV4HLokLsXU0"},"source":["# Sentiment Analysis with VADER"]},{"cell_type":"markdown","metadata":{"id":"DqNgslwWsXU2"},"source":["## Scrape Comments from Reddit Using Pushshift and PRAW"]},{"cell_type":"code","metadata":{"id":"vwYGGgYHsXU2"},"source":["#function to get data from pushshift api\n","def getPushshiftData(query, after, before, sub):\n","    url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n","    print(url)\n","    r = requests.get(url)\n","    data = json.loads(r.text)\n","    return data['data']\n","\n","#get relevant data from data extracted using previous function\n","def collectSubData(subm):\n","    subData = [subm['id'], subm['title'], subm['url'], datetime.datetime.fromtimestamp(subm['created_utc']).date()]\n","    try:\n","        flair = subm['link_flair_text']\n","    except KeyError:\n","        flair = \"NaN\"\n","    subData.append(flair)\n","    subStats.append(subData)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iMWnxL0fuqEs"},"source":["after_date = \"04/01/2018\"\n","before_date = \"23/03/2021\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D9DxRfQ8sXU3"},"source":["#Subreddit to query\n","sub='wallstreetbets'\n","#before and after dates\n","before = str(int(time.mktime(datetime.datetime.strptime(b_date, \"%d/%m/%Y\").timetuple()))) #july 10 2020\n","after = str(int(time.mktime(datetime.datetime.strptime(a_date, \"%d/%m/%Y\").timetuple()))) #july 1 2017\n","#query string\n","query = \"Daily Discussion\"\n","subCount = 0\n","subStats = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JoKL3NmhsXU4","scrolled":true},"source":["data = getPushshiftData(query, after, before, sub)\n","# Will run until all posts have been gathered \n","# from the 'after' date up until before date\n","while len(data) > 0:\n","    for submission in data:\n","        collectSubData(submission)\n","        subCount+=1\n","    # Calls getPushshiftData() with the created date of the last submission\n","    print(len(data))\n","    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n","    after = data[-1]['created_utc']\n","    try:\n","        data = getPushshiftData(query, after, before, sub)\n","    except Exception as e:\n","        print(e)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UZnUaOt-sXU4"},"source":["#organize data into dataframe\n","data={}\n","ids=[]\n","titles=[]\n","urls=[]\n","dates=[]\n","flairs=[]\n","for stat in subStats:\n","    ids.append(stat[0])\n","    titles.append(stat[1])\n","    urls.append(stat[2])\n","    dates.append(stat[3])\n","    flairs.append(stat[4])\n","data['id']=ids\n","data['title']=titles\n","data['url']=urls\n","data['date']=dates\n","data['flair']=flairs\n","df_1=pd.DataFrame(data)\n","df_1=df_1[(df_1['flair']=='Daily Discussion')]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4O3qyATVGYcD"},"source":["## Download data from Reddit using praw"]},{"cell_type":"code","metadata":{"id":"KhDsxRpbsXU5"},"source":["#connect to reddit api\n","reddit = praw.Reddit(client_id='1XambE_tem5SOw',\n","                     client_secret='F5ihhavbKJQ_-E5aLMw5OgAZ-bAz4w', \n","                     user_agent='example')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zj5zK56jsXU5"},"source":["#collect comments using praw\n","comments_by_day=[]\n","for url in tqdm(df_1['url'].tolist()):\n","    try:\n","        submission = reddit.submission(url=url)\n","        submission.comments.replace_more(limit=0)\n","        comments=list([(comment.body) for comment in submission.comments])\n","    except:\n","        comments=None\n","    comments_by_day.append(comments)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8vG2EdUvGgoe"},"source":["## Symbol filtering"]},{"cell_type":"markdown","metadata":{"id":"D7fEtl0WHj5E"},"source":["Use some stop words that might create ambiguity with stock names in comments"]},{"cell_type":"code","metadata":{"id":"wtbKNE74riFz"},"source":["!wget https://gist.githubusercontent.com/ZohebAbai/513218c3468130eacff6481f424e4e64/raw/b70776f341a148293ff277afa0d0302c8c38f7e2/gist_stopwords.txt\n","\n","gist_file = open(\"gist_stopwords.txt\", \"r\")\n","try:\n","    content = gist_file.read()\n","    stop_words = content.split(\",\")\n","finally:\n","    gist_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jr4S85hiriAM"},"source":["#Add more stop words that are used in the discussions\n","stop_words += ['ATH', 'SAVE', 'US', 'ALL', 'LOVE', 'FOR', 'ME', \n","               'GET', \"BEAT\", 'JACK', \"PUMP\", \"BIG\", \"KIDS\", 'STAY', \n","               'TRUE', 'EDIT','PLAY', \"ROCK\", \"NICE\", \"DIE\", \"COST\", \n","               \"WORK\", \"MF\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nh8GQ-wrHBzF"},"source":["- Remove stocks with only nubers as their mention will be next to none,\n","\n","- Remove symbols with length smaller than 2 to reduce similarities between common English words."]},{"cell_type":"code","metadata":{"id":"ATmsBcW3jzYj"},"source":["filter_init = bloomberg_tickers#[bloomberg_tickers.apply(lambda x: (x.split(\" \")[1] == \"US\") | (x.split(\" \")[1] == \"CA\"))]\n","filter_init = filter_init.apply(lambda x: x.split(\" \")[0])\n","ticks = filter_init[filter_init.str.len()>=2].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FBRAP-PQK2ZT"},"source":["ticks = [t for t in ticks if not str.isdigit(t)]\n","ticks = [t for t in ticks if t not in stop_words]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ixpt4bwwBtDO"},"source":["\"BB\" in ticks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6_Jig8J3A9EP"},"source":["#Assert tick is not in stop words\n","ticks_ = []\n","for tic in ticks:\n","    if tic.lower() not in stop_words:\n","        ticks_.append(tic)\n","\n","ticks = ticks_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CEaaKmqEMjTF"},"source":["np.intersect1d(ticks, [s.upper() for s in stop_words])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_D3ByN0OHtdj"},"source":["## **Idea is:**\n","\n","1. Score all comments for a day based on sentiment\n","\n","2. Log all tickers mentioned in those comments\n","\n","3. Assign the daily sentiment to tickers involved"]},{"cell_type":"markdown","metadata":{"id":"FovNdK42fhxq"},"source":["## Run VADER Analysis"]},{"cell_type":"code","metadata":{"id":"7a07-RYM_0iS"},"source":["analyser = SentimentIntensityAnalyzer()\n","\n","scores = [] #For entire market\n","daily_tick_sentiments = [] #array of dictionaries with daily ticker sentiments\n","\n","for comments in tqdm(comments_by_day):\n","    sentiment_score = 0\n","    c_tickers = []\n","    ticks_sent = dict()          #daily sentiments for tickers\n","    for tick in ticks:\n","        ticks_sent[tick] = 0     #initializing with 0\n","    try:\n","        for comment in comments:\n","\n","            ticks_in_comment = []    \n","            for tick in ticks:\n","                #Scanning for ticks mentioned in the comment\n","                if (\" \" + tick + \" \" in comment) and (tick.lower() not in stop_words):\n","                    ticks_in_comment.append(tick)\n","\n","            comment_score = analyser.polarity_scores(comment)[\"compound\"]  #general score\n","\n","            for tick in ticks_in_comment:\n","                #updating the scores of comment to all ticks in the comment\n","                ticks_sent[tick] = comment_score + ticks_sent[tick]\n","            sentiment_score = sentiment_score + comment_score\n","\n","        daily_tick_sentiments.append(ticks_sent) \n","    except TypeError:\n","        sentiment_score = 0\n","\n","    scores.append(sentiment_score)\n","\n","df_1[\"sentiment score\"] = scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"620ynRzdM6Il"},"source":["daily_arr = []\n","for day in daily_tick_sentiments:\n","    daily_arr.append(pd.Series(day))\n","\n","day_df = pd.concat(daily_arr, 1)\n","newvals = df_1.date.values[:len(df_1.date.values)-3]\n","\n","day_df.columns = newvals"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"olmeJcPTbIIK"},"source":["day_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p69dhCQBtpgp"},"source":["#select 400 extreme symbols as theere are only a few symbols \n","#that are discussed daily\n","top_up = day_df.sum(1).nlargest(200).index\n","top_down = day_df.sum(1).nsmallest(200).index\n","\n","tickers = top_down.append(top_up)\n","tickers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8TpAlTqFIwEq"},"source":["1. Transpose, \n","2. calculate rolling average\n","3. Transpose"]},{"cell_type":"code","metadata":{"id":"XrFq6TOzvTPa"},"source":["ps = day_df[day_df.columns[:]].T.rolling(window=14).sum().T #Rolling sum over all dates\n","\n","ps = ps.iloc[:, -1] #Scores for yesterday\n","#ps = ps.iloc[:, -7:].sum(1) #This can also be used\n","\n","#ps = ps[ps!=0] #Remove tickers with no 0 sentiment scores\n","ps = ps.loc[tickers] #Or choose specific tickers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U8xUWe9_1Ehx"},"source":["scores = ps.rank(pct=True).sort_values(ascending=False).reset_index()\n","scores.columns = [\"bloomberg_ticker\", \"signal\"]\n","scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lh0sG7kkJpn9"},"source":["map shortned symbols back to bloomberg symbols"]},{"cell_type":"code","metadata":{"id":"nzx9HrJe1gD7"},"source":["mapping = pd.Series(\n","    bloomberg_tickers.values, index=bloomberg_tickers.apply(lambda x: x.split(\" \")[0])\n",")\n","scores[\"bloomberg_ticker\"] = scores[\"bloomberg_ticker\"].apply(\n","    lambda x: mapping[x] if type(mapping[x]) == str else mapping[x].values[0]\n",")\n","scores.set_index(\"bloomberg_ticker\").to_csv(\"Signal_WSB_ema.csv\", index=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V5XjSL3VsXVF"},"source":["## merge with spy price and plot\n","\n","spy=ffn.get('spy', start='2010-01-01')\n","spy_vals=[]\n","for date in tqdm(df_1['date'].astype(str).values):\n","    try:\n","        spy_vals.append(float(spy.loc[date]))\n","    except KeyError:\n","        spy_vals.append(None)\n","        \n","df_1['spy']=spy_vals"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S0cBnAwfsXVG"},"source":["df_1=df_1[['date','sentiment score','spy']]\n","df_1=df_1.set_index('date')\n","df_1=df_1[df_1['spy'].notna()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VP0PrvYasXVG"},"source":["df_1.plot(secondary_y='sentiment score', figsize=(16, 10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yTRTvCEhsXVG"},"source":["## fourier transform\n","\n","close_fft = np.fft.fft(np.asarray(df_1['sentiment score'].tolist()))\n","fft_df = pd.DataFrame({'fft':close_fft})\n","fft_df['absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))\n","fft_df['angle'] = fft_df['fft'].apply(lambda x: np.angle(x))\n","fft_list = np.asarray(fft_df['fft'].tolist())\n","\n","for num_ in [5, 10, 15, 20]:\n","    fft_list_m10= np.copy(fft_list); fft_list_m10[num_:-num_]=0\n","    df_1['fourier '+str(num_)]=np.fft.ifft(fft_list_m10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"flparmbPsXVH"},"source":["df_1[['sentiment score', 'fourier 5', 'fourier 10', 'fourier 15', 'fourier 20']].plot(figsize=(16, 10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1mTbkLTTsXVH"},"source":["df_1[['spy', 'fourier 20']].plot(secondary_y='fourier 20', figsize=(16, 10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sgGn-IJBsXVI"},"source":["#normalize\n","from sklearn.preprocessing import MinMaxScaler\n","sc= MinMaxScaler(feature_range=(0,1))\n","df_1['norm_price']=sc.fit_transform(df_1['spy'].to_numpy().reshape(-1, 1))\n","df_1['spy log']=np.log(df_1['spy']/df_1['spy'].shift(1))\n","df_1['norm_sentiment']=sc.fit_transform(df_1['sentiment score'].to_numpy().reshape(-1, 1))\n","df_1['norm_fourier5']=sc.fit_transform(np.asarray(list([(float(x)) for x in df_1['fourier 5'].to_numpy()])).reshape(-1, 1))\n","df_1['norm_fourier10']=sc.fit_transform(np.asarray(list([(float(x)) for x in df_1['fourier 10'].to_numpy()])).reshape(-1, 1))\n","df_1['norm_fourier15']=sc.fit_transform(np.asarray(list([(float(x)) for x in df_1['fourier 15'].to_numpy()])).reshape(-1, 1))\n","df_1['norm_fourier20']=sc.fit_transform(np.asarray(list([(float(x)) for x in df_1['fourier 20'].to_numpy()])).reshape(-1, 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LxvnAgbssXVI"},"source":["df_1[['norm_price', 'norm_sentiment', 'norm_fourier5', 'norm_fourier20']].plot(figsize=(16, 10));"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9QoS8k-1_J8e"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DR3N8ZDKKRXD"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"zC4uFhF5KSTA"},"source":["### This notebook is built upon the works of,\n","\n","1.   [Can we actually predict market change by analyzing Redditâ€™s /r/wallstreetbets?](https://medium.com/the-innovation/can-we-actually-predict-market-change-by-analyzing-reddits-r-wallstreetbets-9d7716516c8e)\n","2.   [Sentiment Analysis for Trading with Reddit Text Data](https://medium.com/analytics-vidhya/sentiment-analysis-for-trading-with-reddit-text-data-73729c931d01)\n","\n"]},{"cell_type":"code","metadata":{"id":"HU_L7IWdKRuz"},"source":[""],"execution_count":null,"outputs":[]}]}